{
  "title": "Retrieval Augmented Generation (RAG): The Solution to GenAI Hallucinations",
  "publishedAt": "2023-11-11T00:00:00.000Z",
  "updatedAt": "2023-11-12T00:00:00.000Z",
  "description": "Now that your vector database contains numerical representations of your target data, you can perform a semantic search. Vector databases shine in semantic search use cases because end users form queries with ambiguous natural language.",
  "image": {
    "filePath": "../public/blogs/LLM Knowledge.webp",
    "relativeFilePath": "../../public/blogs/LLM Knowledge.webp",
    "format": "webp",
    "height": 397,
    "width": 720,
    "aspectRatio": 1.81360201511335,
    "blurhashDataUrl": "data:image/webp;base64,UklGRkQAAABXRUJQVlA4IDgAAADQAQCdASoIAAgAAkA4JYwCdAEPAt2/0AD+/CQSsqwiAO8tR9gsfIBv/z31Q4GbJtM9KBS5qRnAAA=="
  },
  "isPublished": true,
  "author": "Mark Sikaundi",
  "tags": [
    "llm"
  ],
  "body": {
    "raw": "\nProducts built on top of Large Language Models (LLMs) such as OpenAI's ChatGPT and Anthropic's Claude are brilliant yet flawed. Powerful as they are, current LLMs suffer from several drawbacks:\n\n- They are static - LLMs are “frozen in time” and lack up-to-date information. It is not feasible to update their gigantic training datasets.\n- They lack domain-specific knowledge - LLMs are trained for generalized tasks, meaning they do not know your company’s private data.\n- They function as “black boxes” - it’s not easy to understand which sources an LLM was considering when they arrived at their conclusions.\n- They are inefficient and costly to produce - Few organizations have the financial and human resources to produce and deploy foundation models.\n\n> Unfortunately, these issues affect the accuracy of GenAI applications leveraging LLMs. For any business application with requirements more demanding than your average chatbot demo, LLMs used “out of the box” with no modifications aside from prompting will perform poorly at context-dependent tasks, such as helping customers book their next flight.\n\nThis post will examine why Retrieval Augmented Generation is the preferred method for addressing these drawbacks, provide a deep dive into how RAG works, and explain why most companies use RAG to boost their GenAI applications’ performance.\n\n### LLMs are “stuck” at a particular time, but RAG can bring them into the present.\n\nChatGPT’s training data “cutoff point” was September 2021. If you ask ChatGPT about something that occurred last month, it will not only fail to answer your question factually; it will likely dream up a very convincing-sounding outright lie. We commonly refer to this behavior as “hallucination.”\n\nLLMs frequently hallucinate when asked about context not within their training dataset\nThe LLM lacks domain-specific information about the Volvo XC60 in the above example. Although the LLM has no idea how to turn off reverse braking for that car model, it performs its generative task to the best of its ability anyway, producing an answer that sounds grammatically solid - but is unfortunately flatly incorrect.\n\nThe reason LLMs like ChatGPT feel so bright is that they've seen an immense amount of human creative output - entire companies’ worth of open source code, libraries worth of books, lifetimes of conversations, scientific datasets, etc., but, critically, this core training data is static.\n\nAfter OpenAI finished training the foundation model (FM) behind ChatGPT in 2021, it received no additional updated information about the world; it remains oblivious to significant world events, weather systems, new laws, the outcomes of sporting events, and, as demonstrated in the image above, the operating procedures for the latest automobiles.\n\n### RAG provides up-to-date information about the world and domain-specific data to your GenAI applications.\n\nRetrieval Augmented Generation means fetching up-to-date or context-specific data from an external database and making it available to an LLM when asking it to generate a response, solving this problem. You can store proprietary business data or information about the world and have your application fetch it for the LLM at generation time, reducing the likelihood of hallucinations. The result is a noticeable boost in the performance and accuracy of your GenAI application.\n\n### LLMs lack context from private data - leading to hallucinations when asked domain or company-specific questions.\n\nThe second major drawback of current LLMs is that, although their base corpus of knowledge is impressive, they do not know the specifics of your business, your requirements, your customer base, or the context your application is running in - such as your e-commerce store.\n\nRAG addresses this second issue by providing extra context and factual information to your GenAI application’s LLM at generation time: anything from customer records to paragraphs of dialogue in a play, to product specifications and current stock, to audio such as voice or songs. The LLM uses this provided content to generate an informed answer.\n\n### Retrieval Augmented Generation allows GenAI to cite its sources and improves auditability.\n\nIn addition to addressing the recency and domain-specific data issues, RAG also allows GenAI applications to provide their sources, much like research papers will provide citations for where they obtained an essential piece of data used in their findings.\n\nImagine a GenAI application serving the legal industry by helping lawyers prepare arguments. The GenAI application will ultimately present its recommendations as final outputs. Still, RAG enables it to provide citations of the legal precedents, local laws, and the evidence it used when arriving at its proposals.\n\nRAG makes the inner workings of GenAI applications easier to audit and understand. It allows end users to jump straight into the same source documents the LLM used when creating its answers.\n\n### Why is RAG the preferred approach from a cost-efficacy perspective?\n\nThere are three main options for improving the performance of your GenAI application other than RAG. Let’s examine them to understand why RAG remains the main path most companies take today.\n\n#### Create your own foundation model\n\nOpenAI’s Sam Altman estimated it cost around $100 million to train the foundation model behind ChatGPT.\n\nNot every company or model will require such a significant investment, but ChatGPT’s price tag underscores that cost is a real challenge in producing sophisticated models with today’s techniques.\n\nIn addition to raw compute costs, you’ll also face the scarce talent issue: you need specialized teams of machine learning PhDs, top-notch systems engineers, and highly skilled operations folks to tackle the many technical challenges of producing such a model and every other AI company in the world is angling for the same rare talent.\n\nAnother challenge is obtaining, sanitizing, and labeling the datasets required to produce a capable foundation model. For example, suppose you’re a legal discovery company considering training your model to answer questions about legal documents. In that case, you’ll also need legal experts to spend many hours labeling training data.\n\nEven if you have access to sufficient capital, can assemble the right team, obtain and label adequate datasets and overcome the many technical hurdles to hosting your model in production, there’s no guarantee of success. The industry has seen several ambitious AI startups come and go, and we expect to see more failures.\n\n#### Fine-tuning: adapting a foundation model to your domain’s data.\n\nFine-tuning is the process of retraining a foundation model on new data. It can certainly be cheaper than building a foundation model from scratch. Still, this approach suffers from many of the same downsides of creating a foundation model: you need rare and deep expertise and sufficient data, and the costs and technical complexity of hosting your model in production don’t go away.\n\nFine-tuning is not a practical approach now that LLMs are pairable with vector databases for context retrieval. Some LLM providers, such as OpenAI, no longer support fine-tuning for their latest-generation models.\n\nFine-tuning is an outdated method of improving LLM outputs. It required recurring, costly, and time-intensive labeling work by subject-matter experts and constant monitoring for quality drift, undesirable deviations in the accuracy of a model due to a lack of regular updates, or changes in data distribution.\n\nIf your data changes over time, even a fine-tuned model’s accuracy can drop, requiring more costly and time-intensive data labeling, constant quality monitoring, and repeated fine-tuning.\n\nImagine updating your model every time you sell a car so your GenAI application has the most recent inventory data.\n\n#### Prompt engineering is insufficient for reducing hallucinations.\n\nPrompt engineering means testing and tweaking the instructions you provide your model to attempt to coax it to do what you want.\n\nIt’s also the cheapest option to improve the accuracy of your GenAI application because you can quickly update the instructions provided to your GenAI application’s LLM with a few code changes.\n\nIt refines the responses your LLMs return but cannot provide them with any new or dynamic context, so your GenAI application will still lack up-to-date context and be susceptible to hallucination.\n\n#### Let’s now take a deeper dive into how Retrieval Augmented Generation works.\n\nWe’ve discussed how RAG passes additional relevant content from your domain-specific database to an LLM at generation time, alongside the original prompt or question, through a “context window\".\n\nAn LLM’s context window is its field of vision at a given moment. RAG is like holding up a cue card containing the critical points for your LLM to see, helping it produce more accurate responses incorporating essential data.\n\nTo understand RAG, we must first understand semantic search, which attempts to find the true meaning of the user’s query and retrieve relevant information instead of simply matching keywords in the user’s query. Semantic search aims to deliver results that better fit the user’s intent, not just their exact words.\n\n- Creating a vector database from your domain-specific proprietary data\n- Creating a vector database from your domain-specific proprietary data using an embedding model.\n  This diagram shows how you make a vector database from your domain-specific, proprietary data. To create your vector database, you convert your data into vectors by running it through an embedding model.\n\nAn embedding model is a type of LLM that converts data into vectors: arrays, or groups, of numbers. In the above example, we’re converting user manuals containing the ground truth for operating the latest Volvo vehicle, but your data could be text, images, video, or audio.\n\nThe most important thing to understand is that a vector represents the meaning of the input text, the same way another human would understand the essence if you spoke the text aloud. We convert our data to vectors so that computers can search for semantically similar items based on the numerical representation of the stored data.\n\nNext, you put the vectors into a vector database, like Pinecone. Pinecone’s vector database can search billions of items for similar matches in under a second.\n\nRemember that you can create vectors, ingest the vectors into the database, and update the index in real-time, solving the recency problem for the LLMs in your GenAI applications.\n\nFor example, you can write code that automatically creates vectors for your latest product offering and then upserts them in your index each time you launch a new product. Your company’s support chatbot application can then use RAG to retrieve up-to-date information about product availability and data about the current customer it’s chatting with.\n\n### Vector databases allow you to query data using natural language, which is ideal for chat interfaces.\n\nNow that your vector database contains numerical representations of your target data, you can perform a semantic search. Vector databases shine in semantic search use cases because end users form queries with ambiguous natural language.\n\n### Semantic search works by converting the user’s query into embeddings and using a vector database to search for similar entries.\n\nRetrieval Augmented Generation (RAG) uses semantic search to retrieve relevant context\nRetrieval Augmented Generation (RAG) uses semantic search to retrieve relevant and timely context that LLMs use to produce more accurate responses.\nYou originally converted your proprietary data into embeddings. When the user issues a query or question, you translate their natural language search terms into embeddings.\n\nYou send these embeddings to the vector database. The database performs a “nearest neighbor” search, finding the vectors that most closely resemble the user’s intent. When the vector database returns the relevant results, your application provides them to the LLM via its context window, prompting it to perform its generative task.\n\nRetrieval Augmented Generation reduces the likelihood of hallucinations by providing domain-specific information through an LLM's context window.\nRetrieval Augmented Generation reduces the likelihood of hallucinations by providing domain-specific information through an LLM's context window.\nSince the LLM now has access to the most pertinent and grounding facts from your vector database, it can provide an accurate answer for your user. RAG reduces the likelihood of hallucination.\n\n### Vector databases can support even more advanced search functionality.\n\nSemantic search is powerful, but it’s possible to go even further. For example, Pinecone’s vector database supports hybrid search functionality, a retrieval system that considers the query's semantics and keywords.\n\nRAG is the most cost-effective, easy to implement, and lowest-risk path to higher performance for GenAI applications.\nSemantic search and Retrieval Augmented Generation provide more relevant GenAI responses, translating to a superior experience for end-users. Unlike building your foundation model, fine-tuning an existing model, or solely performing prompt engineering, RAG simultaneously addresses recency and context-specific issues cost-effectively and with lower risk than alternative approaches.\n\n> Its primary purpose is to provide context-sensitive, detailed answers to questions that require access to private data to answer correctly.\n",
    "code": "var Component=(()=>{var un=Object.create;var C=Object.defineProperty;var fn=Object.getOwnPropertyDescriptor;var cn=Object.getOwnPropertyNames;var ln=Object.getPrototypeOf,mn=Object.prototype.hasOwnProperty;var z=(f,n)=>()=>(n||f((n={exports:{}}).exports,n),n.exports),bn=(f,n)=>{for(var h in n)C(f,h,{get:n[h],enumerable:!0})},ke=(f,n,h,v)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let y of cn(n))!mn.call(f,y)&&y!==h&&C(f,y,{get:()=>n[y],enumerable:!(v=fn(n,y))||v.enumerable});return f};var pn=(f,n,h)=>(h=f!=null?un(ln(f)):{},ke(n||!f||!f.__esModule?C(h,\"default\",{value:f,enumerable:!0}):h,f)),hn=f=>ke(C({},\"__esModule\",{value:!0}),f);var Ne=z((xn,xe)=>{xe.exports=React});var we=z(B=>{\"use strict\";(function(){\"use strict\";var f=Ne(),n=Symbol.for(\"react.element\"),h=Symbol.for(\"react.portal\"),v=Symbol.for(\"react.fragment\"),y=Symbol.for(\"react.strict_mode\"),X=Symbol.for(\"react.profiler\"),K=Symbol.for(\"react.provider\"),H=Symbol.for(\"react.context\"),R=Symbol.for(\"react.forward_ref\"),P=Symbol.for(\"react.suspense\"),S=Symbol.for(\"react.suspense_list\"),U=Symbol.for(\"react.memo\"),G=Symbol.for(\"react.lazy\"),Ue=Symbol.for(\"react.offscreen\"),J=Symbol.iterator,Ee=\"@@iterator\";function Ae(e){if(e===null||typeof e!=\"object\")return null;var r=J&&e[J]||e[Ee];return typeof r==\"function\"?r:null}var x=f.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;function m(e){{for(var r=arguments.length,a=new Array(r>1?r-1:0),i=1;i<r;i++)a[i-1]=arguments[i];Te(\"error\",e,a)}}function Te(e,r,a){{var i=x.ReactDebugCurrentFrame,d=i.getStackAddendum();d!==\"\"&&(r+=\"%s\",a=a.concat([d]));var u=a.map(function(s){return String(s)});u.unshift(\"Warning: \"+r),Function.prototype.apply.call(console[e],console,u)}}var Le=!1,Ie=!1,Ce=!1,Pe=!1,Se=!1,Z;Z=Symbol.for(\"react.module.reference\");function Ge(e){return!!(typeof e==\"string\"||typeof e==\"function\"||e===v||e===X||Se||e===y||e===P||e===S||Pe||e===Ue||Le||Ie||Ce||typeof e==\"object\"&&e!==null&&(e.$$typeof===G||e.$$typeof===U||e.$$typeof===K||e.$$typeof===H||e.$$typeof===R||e.$$typeof===Z||e.getModuleId!==void 0))}function Oe(e,r,a){var i=e.displayName;if(i)return i;var d=r.displayName||r.name||\"\";return d!==\"\"?a+\"(\"+d+\")\":a}function Q(e){return e.displayName||\"Context\"}function g(e){if(e==null)return null;if(typeof e.tag==\"number\"&&m(\"Received an unexpected object in getComponentNameFromType(). This is likely a bug in React. Please file an issue.\"),typeof e==\"function\")return e.displayName||e.name||null;if(typeof e==\"string\")return e;switch(e){case v:return\"Fragment\";case h:return\"Portal\";case X:return\"Profiler\";case y:return\"StrictMode\";case P:return\"Suspense\";case S:return\"SuspenseList\"}if(typeof e==\"object\")switch(e.$$typeof){case H:var r=e;return Q(r)+\".Consumer\";case K:var a=e;return Q(a._context)+\".Provider\";case R:return Oe(e,e.render,\"ForwardRef\");case U:var i=e.displayName||null;return i!==null?i:g(e.type)||\"Memo\";case G:{var d=e,u=d._payload,s=d._init;try{return g(s(u))}catch{return null}}}return null}var k=Object.assign,j=0,ee,ne,te,re,ae,ie,oe;function se(){}se.__reactDisabledLog=!0;function Me(){{if(j===0){ee=console.log,ne=console.info,te=console.warn,re=console.error,ae=console.group,ie=console.groupCollapsed,oe=console.groupEnd;var e={configurable:!0,enumerable:!0,value:se,writable:!0};Object.defineProperties(console,{info:e,log:e,warn:e,error:e,group:e,groupCollapsed:e,groupEnd:e})}j++}}function Fe(){{if(j--,j===0){var e={configurable:!0,enumerable:!0,writable:!0};Object.defineProperties(console,{log:k({},e,{value:ee}),info:k({},e,{value:ne}),warn:k({},e,{value:te}),error:k({},e,{value:re}),group:k({},e,{value:ae}),groupCollapsed:k({},e,{value:ie}),groupEnd:k({},e,{value:oe})})}j<0&&m(\"disabledDepth fell below zero. This is a bug in React. Please file an issue.\")}}var O=x.ReactCurrentDispatcher,M;function E(e,r,a){{if(M===void 0)try{throw Error()}catch(d){var i=d.stack.trim().match(/\\n( *(at )?)/);M=i&&i[1]||\"\"}return`\n`+M+e}}var F=!1,A;{var qe=typeof WeakMap==\"function\"?WeakMap:Map;A=new qe}function de(e,r){if(!e||F)return\"\";{var a=A.get(e);if(a!==void 0)return a}var i;F=!0;var d=Error.prepareStackTrace;Error.prepareStackTrace=void 0;var u;u=O.current,O.current=null,Me();try{if(r){var s=function(){throw Error()};if(Object.defineProperty(s.prototype,\"props\",{set:function(){throw Error()}}),typeof Reflect==\"object\"&&Reflect.construct){try{Reflect.construct(s,[])}catch(_){i=_}Reflect.construct(e,[],s)}else{try{s.call()}catch(_){i=_}e.call(s.prototype)}}else{try{throw Error()}catch(_){i=_}e()}}catch(_){if(_&&i&&typeof _.stack==\"string\"){for(var o=_.stack.split(`\n`),b=i.stack.split(`\n`),c=o.length-1,l=b.length-1;c>=1&&l>=0&&o[c]!==b[l];)l--;for(;c>=1&&l>=0;c--,l--)if(o[c]!==b[l]){if(c!==1||l!==1)do if(c--,l--,l<0||o[c]!==b[l]){var p=`\n`+o[c].replace(\" at new \",\" at \");return e.displayName&&p.includes(\"<anonymous>\")&&(p=p.replace(\"<anonymous>\",e.displayName)),typeof e==\"function\"&&A.set(e,p),p}while(c>=1&&l>=0);break}}}finally{F=!1,O.current=u,Fe(),Error.prepareStackTrace=d}var w=e?e.displayName||e.name:\"\",ve=w?E(w):\"\";return typeof e==\"function\"&&A.set(e,ve),ve}function We(e,r,a){return de(e,!1)}function Ye(e){var r=e.prototype;return!!(r&&r.isReactComponent)}function T(e,r,a){if(e==null)return\"\";if(typeof e==\"function\")return de(e,Ye(e));if(typeof e==\"string\")return E(e);switch(e){case P:return E(\"Suspense\");case S:return E(\"SuspenseList\")}if(typeof e==\"object\")switch(e.$$typeof){case R:return We(e.render);case U:return T(e.type,r,a);case G:{var i=e,d=i._payload,u=i._init;try{return T(u(d),r,a)}catch{}}}return\"\"}var L=Object.prototype.hasOwnProperty,ue={},fe=x.ReactDebugCurrentFrame;function I(e){if(e){var r=e._owner,a=T(e.type,e._source,r?r.type:null);fe.setExtraStackFrame(a)}else fe.setExtraStackFrame(null)}function Ve(e,r,a,i,d){{var u=Function.call.bind(L);for(var s in e)if(u(e,s)){var o=void 0;try{if(typeof e[s]!=\"function\"){var b=Error((i||\"React class\")+\": \"+a+\" type `\"+s+\"` is invalid; it must be a function, usually from the `prop-types` package, but received `\"+typeof e[s]+\"`.This often happens because of typos such as `PropTypes.function` instead of `PropTypes.func`.\");throw b.name=\"Invariant Violation\",b}o=e[s](r,s,i,a,null,\"SECRET_DO_NOT_PASS_THIS_OR_YOU_WILL_BE_FIRED\")}catch(c){o=c}o&&!(o instanceof Error)&&(I(d),m(\"%s: type specification of %s `%s` is invalid; the type checker function must return `null` or an `Error` but returned a %s. You may have forgotten to pass an argument to the type checker creator (arrayOf, instanceOf, objectOf, oneOf, oneOfType, and shape all require an argument).\",i||\"React class\",a,s,typeof o),I(null)),o instanceof Error&&!(o.message in ue)&&(ue[o.message]=!0,I(d),m(\"Failed %s type: %s\",a,o.message),I(null))}}}var $e=Array.isArray;function q(e){return $e(e)}function ze(e){{var r=typeof Symbol==\"function\"&&Symbol.toStringTag,a=r&&e[Symbol.toStringTag]||e.constructor.name||\"Object\";return a}}function Be(e){try{return ce(e),!1}catch{return!0}}function ce(e){return\"\"+e}function le(e){if(Be(e))return m(\"The provided key is an unsupported type %s. This value must be coerced to a string before before using it here.\",ze(e)),ce(e)}var D=x.ReactCurrentOwner,Xe={key:!0,ref:!0,__self:!0,__source:!0},me,be,W;W={};function Ke(e){if(L.call(e,\"ref\")){var r=Object.getOwnPropertyDescriptor(e,\"ref\").get;if(r&&r.isReactWarning)return!1}return e.ref!==void 0}function He(e){if(L.call(e,\"key\")){var r=Object.getOwnPropertyDescriptor(e,\"key\").get;if(r&&r.isReactWarning)return!1}return e.key!==void 0}function Je(e,r){if(typeof e.ref==\"string\"&&D.current&&r&&D.current.stateNode!==r){var a=g(D.current.type);W[a]||(m('Component \"%s\" contains the string ref \"%s\". Support for string refs will be removed in a future major release. This case cannot be automatically converted to an arrow function. We ask you to manually fix this case by using useRef() or createRef() instead. Learn more about using refs safely here: https://reactjs.org/link/strict-mode-string-ref',g(D.current.type),e.ref),W[a]=!0)}}function Ze(e,r){{var a=function(){me||(me=!0,m(\"%s: `key` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\",r))};a.isReactWarning=!0,Object.defineProperty(e,\"key\",{get:a,configurable:!0})}}function Qe(e,r){{var a=function(){be||(be=!0,m(\"%s: `ref` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\",r))};a.isReactWarning=!0,Object.defineProperty(e,\"ref\",{get:a,configurable:!0})}}var en=function(e,r,a,i,d,u,s){var o={$$typeof:n,type:e,key:r,ref:a,props:s,_owner:u};return o._store={},Object.defineProperty(o._store,\"validated\",{configurable:!1,enumerable:!1,writable:!0,value:!1}),Object.defineProperty(o,\"_self\",{configurable:!1,enumerable:!1,writable:!1,value:i}),Object.defineProperty(o,\"_source\",{configurable:!1,enumerable:!1,writable:!1,value:d}),Object.freeze&&(Object.freeze(o.props),Object.freeze(o)),o};function nn(e,r,a,i,d){{var u,s={},o=null,b=null;a!==void 0&&(le(a),o=\"\"+a),He(r)&&(le(r.key),o=\"\"+r.key),Ke(r)&&(b=r.ref,Je(r,d));for(u in r)L.call(r,u)&&!Xe.hasOwnProperty(u)&&(s[u]=r[u]);if(e&&e.defaultProps){var c=e.defaultProps;for(u in c)s[u]===void 0&&(s[u]=c[u])}if(o||b){var l=typeof e==\"function\"?e.displayName||e.name||\"Unknown\":e;o&&Ze(s,l),b&&Qe(s,l)}return en(e,o,b,d,i,D.current,s)}}var Y=x.ReactCurrentOwner,pe=x.ReactDebugCurrentFrame;function N(e){if(e){var r=e._owner,a=T(e.type,e._source,r?r.type:null);pe.setExtraStackFrame(a)}else pe.setExtraStackFrame(null)}var V;V=!1;function $(e){return typeof e==\"object\"&&e!==null&&e.$$typeof===n}function he(){{if(Y.current){var e=g(Y.current.type);if(e)return`\n\nCheck the render method of \\``+e+\"`.\"}return\"\"}}function tn(e){{if(e!==void 0){var r=e.fileName.replace(/^.*[\\\\\\/]/,\"\"),a=e.lineNumber;return`\n\nCheck your code at `+r+\":\"+a+\".\"}return\"\"}}var ge={};function rn(e){{var r=he();if(!r){var a=typeof e==\"string\"?e:e.displayName||e.name;a&&(r=`\n\nCheck the top-level render call using <`+a+\">.\")}return r}}function _e(e,r){{if(!e._store||e._store.validated||e.key!=null)return;e._store.validated=!0;var a=rn(r);if(ge[a])return;ge[a]=!0;var i=\"\";e&&e._owner&&e._owner!==Y.current&&(i=\" It was passed a child from \"+g(e._owner.type)+\".\"),N(e),m('Each child in a list should have a unique \"key\" prop.%s%s See https://reactjs.org/link/warning-keys for more information.',a,i),N(null)}}function ye(e,r){{if(typeof e!=\"object\")return;if(q(e))for(var a=0;a<e.length;a++){var i=e[a];$(i)&&_e(i,r)}else if($(e))e._store&&(e._store.validated=!0);else if(e){var d=Ae(e);if(typeof d==\"function\"&&d!==e.entries)for(var u=d.call(e),s;!(s=u.next()).done;)$(s.value)&&_e(s.value,r)}}}function an(e){{var r=e.type;if(r==null||typeof r==\"string\")return;var a;if(typeof r==\"function\")a=r.propTypes;else if(typeof r==\"object\"&&(r.$$typeof===R||r.$$typeof===U))a=r.propTypes;else return;if(a){var i=g(r);Ve(a,e.props,\"prop\",i,e)}else if(r.PropTypes!==void 0&&!V){V=!0;var d=g(r);m(\"Component %s declared `PropTypes` instead of `propTypes`. Did you misspell the property assignment?\",d||\"Unknown\")}typeof r.getDefaultProps==\"function\"&&!r.getDefaultProps.isReactClassApproved&&m(\"getDefaultProps is only used on classic React.createClass definitions. Use a static property named `defaultProps` instead.\")}}function on(e){{for(var r=Object.keys(e.props),a=0;a<r.length;a++){var i=r[a];if(i!==\"children\"&&i!==\"key\"){N(e),m(\"Invalid prop `%s` supplied to `React.Fragment`. React.Fragment can only have `key` and `children` props.\",i),N(null);break}}e.ref!==null&&(N(e),m(\"Invalid attribute `ref` supplied to `React.Fragment`.\"),N(null))}}function sn(e,r,a,i,d,u){{var s=Ge(e);if(!s){var o=\"\";(e===void 0||typeof e==\"object\"&&e!==null&&Object.keys(e).length===0)&&(o+=\" You likely forgot to export your component from the file it's defined in, or you might have mixed up default and named imports.\");var b=tn(d);b?o+=b:o+=he();var c;e===null?c=\"null\":q(e)?c=\"array\":e!==void 0&&e.$$typeof===n?(c=\"<\"+(g(e.type)||\"Unknown\")+\" />\",o=\" Did you accidentally export a JSX literal instead of a component?\"):c=typeof e,m(\"React.jsx: type is invalid -- expected a string (for built-in components) or a class/function (for composite components) but got: %s.%s\",c,o)}var l=nn(e,r,a,d,u);if(l==null)return l;if(s){var p=r.children;if(p!==void 0)if(i)if(q(p)){for(var w=0;w<p.length;w++)ye(p[w],e);Object.freeze&&Object.freeze(p)}else m(\"React.jsx: Static children should always be an array. You are likely explicitly calling React.jsxs or React.jsxDEV. Use the Babel transform instead.\");else ye(p,e)}return e===v?on(l):an(l),l}}var dn=sn;B.Fragment=v,B.jsxDEV=dn})()});var De=z((wn,je)=>{\"use strict\";je.exports=we()});var vn={};bn(vn,{default:()=>yn,frontmatter:()=>gn});var t=pn(De()),gn={title:\"Retrieval Augmented Generation (RAG): The Solution to GenAI Hallucinations\",description:\"Now that your vector database contains numerical representations of your target data, you can perform a semantic search. Vector databases shine in semantic search use cases because end users form queries with ambiguous natural language.\",image:\"../../public/blogs/LLM Knowledge.webp\",publishedAt:\"2023-11-11\",updatedAt:\"2023-11-12\",author:\"Mark Sikaundi\",isPublished:!0,tags:[\"llm\"]};function Re(f){let n=Object.assign({p:\"p\",ul:\"ul\",li:\"li\",blockquote:\"blockquote\",h3:\"h3\",a:\"a\",span:\"span\",h4:\"h4\"},f.components);return(0,t.jsxDEV)(t.Fragment,{children:[(0,t.jsxDEV)(n.p,{children:\"Products built on top of Large Language Models (LLMs) such as OpenAI's ChatGPT and Anthropic's Claude are brilliant yet flawed. Powerful as they are, current LLMs suffer from several drawbacks:\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:13,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.ul,{children:[`\n`,(0,t.jsxDEV)(n.li,{children:\"They are static - LLMs are \\u201Cfrozen in time\\u201D and lack up-to-date information. It is not feasible to update their gigantic training datasets.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:15,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.li,{children:\"They lack domain-specific knowledge - LLMs are trained for generalized tasks, meaning they do not know your company\\u2019s private data.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:16,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.li,{children:\"They function as \\u201Cblack boxes\\u201D - it\\u2019s not easy to understand which sources an LLM was considering when they arrived at their conclusions.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:17,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.li,{children:\"They are inefficient and costly to produce - Few organizations have the financial and human resources to produce and deploy foundation models.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:18,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:15,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.blockquote,{children:[`\n`,(0,t.jsxDEV)(n.p,{children:\"Unfortunately, these issues affect the accuracy of GenAI applications leveraging LLMs. For any business application with requirements more demanding than your average chatbot demo, LLMs used \\u201Cout of the box\\u201D with no modifications aside from prompting will perform poorly at context-dependent tasks, such as helping customers book their next flight.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:20,columnNumber:3},this),`\n`]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:20,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"This post will examine why Retrieval Augmented Generation is the preferred method for addressing these drawbacks, provide a deep dive into how RAG works, and explain why most companies use RAG to boost their GenAI applications\\u2019 performance.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:22,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h3,{id:\"llms-are-stuck-at-a-particular-time-but-rag-can-bring-them-into-the-present\",children:[\"LLMs are \\u201Cstuck\\u201D at a particular time, but RAG can bring them into the present.\",(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#llms-are-stuck-at-a-particular-time-but-rag-can-bring-them-into-the-present\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:24,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"ChatGPT\\u2019s training data \\u201Ccutoff point\\u201D was September 2021. If you ask ChatGPT about something that occurred last month, it will not only fail to answer your question factually; it will likely dream up a very convincing-sounding outright lie. We commonly refer to this behavior as \\u201Challucination.\\u201D\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:26,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:`LLMs frequently hallucinate when asked about context not within their training dataset\nThe LLM lacks domain-specific information about the Volvo XC60 in the above example. Although the LLM has no idea how to turn off reverse braking for that car model, it performs its generative task to the best of its ability anyway, producing an answer that sounds grammatically solid - but is unfortunately flatly incorrect.`},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:28,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"The reason LLMs like ChatGPT feel so bright is that they've seen an immense amount of human creative output - entire companies\\u2019 worth of open source code, libraries worth of books, lifetimes of conversations, scientific datasets, etc., but, critically, this core training data is static.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:31,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"After OpenAI finished training the foundation model (FM) behind ChatGPT in 2021, it received no additional updated information about the world; it remains oblivious to significant world events, weather systems, new laws, the outcomes of sporting events, and, as demonstrated in the image above, the operating procedures for the latest automobiles.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:33,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h3,{id:\"rag-provides-up-to-date-information-about-the-world-and-domain-specific-data-to-your-genai-applications\",children:[\"RAG provides up-to-date information about the world and domain-specific data to your GenAI applications.\",(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#rag-provides-up-to-date-information-about-the-world-and-domain-specific-data-to-your-genai-applications\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:35,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Retrieval Augmented Generation means fetching up-to-date or context-specific data from an external database and making it available to an LLM when asking it to generate a response, solving this problem. You can store proprietary business data or information about the world and have your application fetch it for the LLM at generation time, reducing the likelihood of hallucinations. The result is a noticeable boost in the performance and accuracy of your GenAI application.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:37,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h3,{id:\"llms-lack-context-from-private-data---leading-to-hallucinations-when-asked-domain-or-company-specific-questions\",children:[\"LLMs lack context from private data - leading to hallucinations when asked domain or company-specific questions.\",(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#llms-lack-context-from-private-data---leading-to-hallucinations-when-asked-domain-or-company-specific-questions\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:39,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"The second major drawback of current LLMs is that, although their base corpus of knowledge is impressive, they do not know the specifics of your business, your requirements, your customer base, or the context your application is running in - such as your e-commerce store.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:41,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"RAG addresses this second issue by providing extra context and factual information to your GenAI application\\u2019s LLM at generation time: anything from customer records to paragraphs of dialogue in a play, to product specifications and current stock, to audio such as voice or songs. The LLM uses this provided content to generate an informed answer.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:43,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h3,{id:\"retrieval-augmented-generation-allows-genai-to-cite-its-sources-and-improves-auditability\",children:[\"Retrieval Augmented Generation allows GenAI to cite its sources and improves auditability.\",(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#retrieval-augmented-generation-allows-genai-to-cite-its-sources-and-improves-auditability\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:45,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"In addition to addressing the recency and domain-specific data issues, RAG also allows GenAI applications to provide their sources, much like research papers will provide citations for where they obtained an essential piece of data used in their findings.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:47,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Imagine a GenAI application serving the legal industry by helping lawyers prepare arguments. The GenAI application will ultimately present its recommendations as final outputs. Still, RAG enables it to provide citations of the legal precedents, local laws, and the evidence it used when arriving at its proposals.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:49,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"RAG makes the inner workings of GenAI applications easier to audit and understand. It allows end users to jump straight into the same source documents the LLM used when creating its answers.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:51,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h3,{id:\"why-is-rag-the-preferred-approach-from-a-cost-efficacy-perspective\",children:[\"Why is RAG the preferred approach from a cost-efficacy perspective?\",(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#why-is-rag-the-preferred-approach-from-a-cost-efficacy-perspective\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:53,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"There are three main options for improving the performance of your GenAI application other than RAG. Let\\u2019s examine them to understand why RAG remains the main path most companies take today.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:55,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h4,{id:\"create-your-own-foundation-model\",children:[\"Create your own foundation model\",(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#create-your-own-foundation-model\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:57,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"OpenAI\\u2019s Sam Altman estimated it cost around $100 million to train the foundation model behind ChatGPT.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:59,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Not every company or model will require such a significant investment, but ChatGPT\\u2019s price tag underscores that cost is a real challenge in producing sophisticated models with today\\u2019s techniques.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:61,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"In addition to raw compute costs, you\\u2019ll also face the scarce talent issue: you need specialized teams of machine learning PhDs, top-notch systems engineers, and highly skilled operations folks to tackle the many technical challenges of producing such a model and every other AI company in the world is angling for the same rare talent.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:63,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Another challenge is obtaining, sanitizing, and labeling the datasets required to produce a capable foundation model. For example, suppose you\\u2019re a legal discovery company considering training your model to answer questions about legal documents. In that case, you\\u2019ll also need legal experts to spend many hours labeling training data.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:65,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Even if you have access to sufficient capital, can assemble the right team, obtain and label adequate datasets and overcome the many technical hurdles to hosting your model in production, there\\u2019s no guarantee of success. The industry has seen several ambitious AI startups come and go, and we expect to see more failures.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:67,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h4,{id:\"fine-tuning-adapting-a-foundation-model-to-your-domains-data\",children:[\"Fine-tuning: adapting a foundation model to your domain\\u2019s data.\",(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#fine-tuning-adapting-a-foundation-model-to-your-domains-data\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:69,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Fine-tuning is the process of retraining a foundation model on new data. It can certainly be cheaper than building a foundation model from scratch. Still, this approach suffers from many of the same downsides of creating a foundation model: you need rare and deep expertise and sufficient data, and the costs and technical complexity of hosting your model in production don\\u2019t go away.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:71,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Fine-tuning is not a practical approach now that LLMs are pairable with vector databases for context retrieval. Some LLM providers, such as OpenAI, no longer support fine-tuning for their latest-generation models.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:73,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Fine-tuning is an outdated method of improving LLM outputs. It required recurring, costly, and time-intensive labeling work by subject-matter experts and constant monitoring for quality drift, undesirable deviations in the accuracy of a model due to a lack of regular updates, or changes in data distribution.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:75,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"If your data changes over time, even a fine-tuned model\\u2019s accuracy can drop, requiring more costly and time-intensive data labeling, constant quality monitoring, and repeated fine-tuning.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:77,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Imagine updating your model every time you sell a car so your GenAI application has the most recent inventory data.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:79,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h4,{id:\"prompt-engineering-is-insufficient-for-reducing-hallucinations\",children:[\"Prompt engineering is insufficient for reducing hallucinations.\",(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#prompt-engineering-is-insufficient-for-reducing-hallucinations\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:81,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Prompt engineering means testing and tweaking the instructions you provide your model to attempt to coax it to do what you want.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:83,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"It\\u2019s also the cheapest option to improve the accuracy of your GenAI application because you can quickly update the instructions provided to your GenAI application\\u2019s LLM with a few code changes.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:85,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"It refines the responses your LLMs return but cannot provide them with any new or dynamic context, so your GenAI application will still lack up-to-date context and be susceptible to hallucination.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:87,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h4,{id:\"lets-now-take-a-deeper-dive-into-how-retrieval-augmented-generation-works\",children:[\"Let\\u2019s now take a deeper dive into how Retrieval Augmented Generation works.\",(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#lets-now-take-a-deeper-dive-into-how-retrieval-augmented-generation-works\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:89,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:'We\\u2019ve discussed how RAG passes additional relevant content from your domain-specific database to an LLM at generation time, alongside the original prompt or question, through a \\u201Ccontext window\".'},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:91,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"An LLM\\u2019s context window is its field of vision at a given moment. RAG is like holding up a cue card containing the critical points for your LLM to see, helping it produce more accurate responses incorporating essential data.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:93,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"To understand RAG, we must first understand semantic search, which attempts to find the true meaning of the user\\u2019s query and retrieve relevant information instead of simply matching keywords in the user\\u2019s query. Semantic search aims to deliver results that better fit the user\\u2019s intent, not just their exact words.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:95,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.ul,{children:[`\n`,(0,t.jsxDEV)(n.li,{children:\"Creating a vector database from your domain-specific proprietary data\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:97,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.li,{children:`Creating a vector database from your domain-specific proprietary data using an embedding model.\nThis diagram shows how you make a vector database from your domain-specific, proprietary data. To create your vector database, you convert your data into vectors by running it through an embedding model.`},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:98,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:97,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"An embedding model is a type of LLM that converts data into vectors: arrays, or groups, of numbers. In the above example, we\\u2019re converting user manuals containing the ground truth for operating the latest Volvo vehicle, but your data could be text, images, video, or audio.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:101,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"The most important thing to understand is that a vector represents the meaning of the input text, the same way another human would understand the essence if you spoke the text aloud. We convert our data to vectors so that computers can search for semantically similar items based on the numerical representation of the stored data.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:103,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Next, you put the vectors into a vector database, like Pinecone. Pinecone\\u2019s vector database can search billions of items for similar matches in under a second.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:105,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Remember that you can create vectors, ingest the vectors into the database, and update the index in real-time, solving the recency problem for the LLMs in your GenAI applications.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:107,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"For example, you can write code that automatically creates vectors for your latest product offering and then upserts them in your index each time you launch a new product. Your company\\u2019s support chatbot application can then use RAG to retrieve up-to-date information about product availability and data about the current customer it\\u2019s chatting with.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:109,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h3,{id:\"vector-databases-allow-you-to-query-data-using-natural-language-which-is-ideal-for-chat-interfaces\",children:[\"Vector databases allow you to query data using natural language, which is ideal for chat interfaces.\",(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#vector-databases-allow-you-to-query-data-using-natural-language-which-is-ideal-for-chat-interfaces\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:111,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Now that your vector database contains numerical representations of your target data, you can perform a semantic search. Vector databases shine in semantic search use cases because end users form queries with ambiguous natural language.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:113,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h3,{id:\"semantic-search-works-by-converting-the-users-query-into-embeddings-and-using-a-vector-database-to-search-for-similar-entries\",children:[\"Semantic search works by converting the user\\u2019s query into embeddings and using a vector database to search for similar entries.\",(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#semantic-search-works-by-converting-the-users-query-into-embeddings-and-using-a-vector-database-to-search-for-similar-entries\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:115,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:`Retrieval Augmented Generation (RAG) uses semantic search to retrieve relevant context\nRetrieval Augmented Generation (RAG) uses semantic search to retrieve relevant and timely context that LLMs use to produce more accurate responses.\nYou originally converted your proprietary data into embeddings. When the user issues a query or question, you translate their natural language search terms into embeddings.`},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:117,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"You send these embeddings to the vector database. The database performs a \\u201Cnearest neighbor\\u201D search, finding the vectors that most closely resemble the user\\u2019s intent. When the vector database returns the relevant results, your application provides them to the LLM via its context window, prompting it to perform its generative task.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:121,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:`Retrieval Augmented Generation reduces the likelihood of hallucinations by providing domain-specific information through an LLM's context window.\nRetrieval Augmented Generation reduces the likelihood of hallucinations by providing domain-specific information through an LLM's context window.\nSince the LLM now has access to the most pertinent and grounding facts from your vector database, it can provide an accurate answer for your user. RAG reduces the likelihood of hallucination.`},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:123,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h3,{id:\"vector-databases-can-support-even-more-advanced-search-functionality\",children:[\"Vector databases can support even more advanced search functionality.\",(0,t.jsxDEV)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#vector-databases-can-support-even-more-advanced-search-functionality\",children:(0,t.jsxDEV)(n.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:127,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Semantic search is powerful, but it\\u2019s possible to go even further. For example, Pinecone\\u2019s vector database supports hybrid search functionality, a retrieval system that considers the query's semantics and keywords.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:129,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:`RAG is the most cost-effective, easy to implement, and lowest-risk path to higher performance for GenAI applications.\nSemantic search and Retrieval Augmented Generation provide more relevant GenAI responses, translating to a superior experience for end-users. Unlike building your foundation model, fine-tuning an existing model, or solely performing prompt engineering, RAG simultaneously addresses recency and context-specific issues cost-effectively and with lower risk than alternative approaches.`},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:131,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.blockquote,{children:[`\n`,(0,t.jsxDEV)(n.p,{children:\"Its primary purpose is to provide context-sensitive, detailed answers to questions that require access to private data to answer correctly.\"},void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:134,columnNumber:3},this),`\n`]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:134,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\",lineNumber:1,columnNumber:1},this)}function _n(f={}){let{wrapper:n}=f.components||{};return n?(0,t.jsxDEV)(n,Object.assign({},f,{children:(0,t.jsxDEV)(Re,f,void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this)}),void 0,!1,{fileName:\"/Users/marksikaundi/Documents/builds/otofast-project-org/content/_mdx_bundler_entry_point-3b2b460f-08cf-4f03-8f2d-6f0a07d0f355.mdx\"},this):Re(f)}var yn=_n;return hn(vn);})();\n/*! Bundled license information:\n\nreact/cjs/react-jsx-dev-runtime.development.js:\n  (**\n   * @license React\n   * react-jsx-dev-runtime.development.js\n   *\n   * Copyright (c) Facebook, Inc. and its affiliates.\n   *\n   * This source code is licensed under the MIT license found in the\n   * LICENSE file in the root directory of this source tree.\n   *)\n*/\n;return Component;"
  },
  "_id": "retrieval-augmented-generation-the-solution-to-genai-hallucinations/index.mdx",
  "_raw": {
    "sourceFilePath": "retrieval-augmented-generation-the-solution-to-genai-hallucinations/index.mdx",
    "sourceFileName": "index.mdx",
    "sourceFileDir": "retrieval-augmented-generation-the-solution-to-genai-hallucinations",
    "contentType": "mdx",
    "flattenedPath": "retrieval-augmented-generation-the-solution-to-genai-hallucinations"
  },
  "type": "Blog",
  "url": "/blogs/retrieval-augmented-generation-the-solution-to-genai-hallucinations",
  "readingTime": {
    "text": "11 min read",
    "minutes": 10.385,
    "time": 623100,
    "words": 2077
  },
  "toc": [
    {
      "level": "three",
      "text": "LLMs are “stuck” at a particular time, but RAG can bring them into the present.",
      "slug": "llms-are-stuck-at-a-particular-time-but-rag-can-bring-them-into-the-present"
    },
    {
      "level": "three",
      "text": "RAG provides up-to-date information about the world and domain-specific data to your GenAI applications.",
      "slug": "rag-provides-up-to-date-information-about-the-world-and-domain-specific-data-to-your-genai-applications"
    },
    {
      "level": "three",
      "text": "LLMs lack context from private data - leading to hallucinations when asked domain or company-specific questions.",
      "slug": "llms-lack-context-from-private-data---leading-to-hallucinations-when-asked-domain-or-company-specific-questions"
    },
    {
      "level": "three",
      "text": "Retrieval Augmented Generation allows GenAI to cite its sources and improves auditability.",
      "slug": "retrieval-augmented-generation-allows-genai-to-cite-its-sources-and-improves-auditability"
    },
    {
      "level": "three",
      "text": "Why is RAG the preferred approach from a cost-efficacy perspective?",
      "slug": "why-is-rag-the-preferred-approach-from-a-cost-efficacy-perspective"
    },
    {
      "level": "three",
      "text": "Create your own foundation model",
      "slug": "create-your-own-foundation-model"
    },
    {
      "level": "three",
      "text": "Fine-tuning: adapting a foundation model to your domain’s data.",
      "slug": "fine-tuning-adapting-a-foundation-model-to-your-domains-data"
    },
    {
      "level": "three",
      "text": "Prompt engineering is insufficient for reducing hallucinations.",
      "slug": "prompt-engineering-is-insufficient-for-reducing-hallucinations"
    },
    {
      "level": "three",
      "text": "Let’s now take a deeper dive into how Retrieval Augmented Generation works.",
      "slug": "lets-now-take-a-deeper-dive-into-how-retrieval-augmented-generation-works"
    },
    {
      "level": "three",
      "text": "Vector databases allow you to query data using natural language, which is ideal for chat interfaces.",
      "slug": "vector-databases-allow-you-to-query-data-using-natural-language-which-is-ideal-for-chat-interfaces"
    },
    {
      "level": "three",
      "text": "Semantic search works by converting the user’s query into embeddings and using a vector database to search for similar entries.",
      "slug": "semantic-search-works-by-converting-the-users-query-into-embeddings-and-using-a-vector-database-to-search-for-similar-entries"
    },
    {
      "level": "three",
      "text": "Vector databases can support even more advanced search functionality.",
      "slug": "vector-databases-can-support-even-more-advanced-search-functionality"
    }
  ]
}